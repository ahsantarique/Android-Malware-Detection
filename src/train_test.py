import os
import math
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, make_scorer
from scipy.stats import pearsonr
from sklearn.neural_network import MLPRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import KFold
import scipy.stats as stat
from sklearn.metrics import f1_score as fscore
import decimal 

def p(y_pred,y_true):
    return pearsonr(y_pred,y_true)[0]


def neuralNet():
	return MLPRegressor(hidden_layer_sizes=(50,100,50), activation='relu', 
                    alpha=0.05, batch_size=128, early_stopping=True, 
                    learning_rate_init=0.01, solver='adam', learning_rate='adaptive', nesterovs_momentum=True, 
                    max_iter=100, tol=1e-8, verbose=False, validation_fraction=0.1)

def svr():
	return SVR(C=0.1, epsilon=0.01)



def splitMetrics(clf, X, y):
	X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9)

	gc.collect()



	clf.fit(X_train, y_train)
	y_test_pred = clf.predict(X_test)
	y_train_pred = clf.predict(X_train)

	gc.collect()

	print(p(y_train, y_train_pred))
	print(mean_squared_error(y_train, y_train_pred))
	print(p(y_test, y_test_pred))



def crossValidate(clf, X, y):
	# score = make_scorer(p, greater_is_better=True)
	scores = cross_val_score(clf, X, y, cv=10, scoring = 'f1')
	# scores = cross_val_score(clf, X, y, cv=10, scoring='neg_mean_squared_error')
	print("*******************")
	print("F1 score of 10 fold CV:", scores)



GOOD_DATA = 'data/good.dat'
BAD_DAT = 'data/bad.dat'

def loadDataSet():
	X0 = np.loadtxt(GOOD_DATA, dtype= np.float128)

	y0 = np.array([0 for i in range(len(X0))])

	X1 = np.loadtxt(BAD_DAT, dtype=np.float128)

	y1 = np.array([1 for i in range(len(X1))])
	
	X = np.append(X0, X1, axis=0)

	y = np.append(y0, y1)
	return X, y


def computeLGD(mean0, mean1, var0, var1):
	array_of_LGD = [0 for i in range(len(mean0))]
	for i in range(len(mean0)):
		if(var0[i] == 0):
			var0[i] = 1
		if(var1[i] == 0):
			var1[i] = 1
		gdi = stat.norm(mean0[i], var0[i]).pdf(mean1[i]) + stat.norm(mean1[i], var1[i]).pdf(mean0[i])

		array_of_LGD[i] = gdi
		# array_of_LGD[i] = -np.log(var0[i]*var1[i]) - ((mean0[i]-mean1[i])**2 * (var0**2+var1**2) / (2*(var0*var1)**2))

	return np.array(array_of_LGD)



def main():
	X, y = loadDataSet()

	kf = KFold(n_splits=10, shuffle=True)
	kf.get_n_splits(X)
	for train_index, test_index in kf.split(X):
	    	# print("TRAIN:", train_index, "TEST:", test_index)

		X_train, X_test = X[train_index], X[test_index]
		y_train, y_test = y[train_index], y[test_index]

		y_train_index0 = np.where(y_train == 0)[0]
		y_train_index1 = np.where(y_train == 1)[0]

		mean0 = np.mean(X_train[y_train_index0], axis=0)
		var0 = np.var(X_train[y_train_index0], axis=0)

		mean1 = np.mean(X_train[y_train_index1], axis=0)
		var1 = np.var(X_train[y_train_index1], axis=0)
		lgd = computeLGD(mean0, mean1, var0, var1)
		sorted_indices = np.argsort(lgd)
		
		with open('sorted_indices.txt', 'a+') as file:
			for i in sorted_indices:
				file.write(str(i)+' ')
			file.write('\n')


		for r in range(1, 2):
		    	selected_feature_indices = sorted_indices[:r]
		    	X_train_selected_features = X_train[:, selected_feature_indices]
		    	X_test_selected_features = X_test[:, selected_feature_indices]

    			clf = GaussianNB()
    			y_test_pred = clf.fit(X_train_selected_features, y_train).predict(X_test_selected_features)

    			f1 = fscore(y_test, y_test_pred)
    			with open('f1score.txt', 'a+') as file:
		    		file.write(str(f1)+'\n')
	# clf = svr() # or 
	# clf = GaussianNB()

	# splitMetrics(clf, X, y) # or 
	# crossValidate(clf, X, y)

	# print(len(y))

	# c = 0.1
	# eps = 0.01
	# # for c,eps in (range(0.1,.5,4)*range(0.01,0.01,3)):
	# clf = SVR(C=c, epsilon=eps)
	# # clf.fit(X[:20], y[:20])

if __name__ == "__main__":
	main()
